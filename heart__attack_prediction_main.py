# -*- coding: utf-8 -*-
"""Heart_ Attack_Prediction_main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10TdIMgB3o8LG12AfzT1ECkHpEu-iyA0z

#STEP 1) DATA LOADING
"""

import os
import pickle
import numpy as np
import pandas as pd
import seaborn as sns
import scipy.stats as ss
import matplotlib.pyplot as plt

from sklearn.svm import SVC
from sklearn import pipeline
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import confusion_matrix, classification_report 
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

def cramers_corrected_stat(confusion_matrix):
    """ calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher,
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))

HA_CSV_PATH = os.path.join(os.getcwd(), 'heart.csv')
ha_df = pd.read_csv(HA_CSV_PATH)

"""#STEP 2) DATA VISUALIZATION / INSPECTION"""

ha_df.head()

ha_df.info()

"""CHECK DATA DISTRIBUTION"""

ha_df.describe().T

ha_df['thall'] = ha_df['thall'].replace(0,np.nan) 
ha_df['caa'] = ha_df['caa'].replace(4,np.nan)

"""This will replace 0 and 4 to NaNs

CHECK NaNs
"""

ha_df.isna().sum()

"""We can observe that there are 7 NaNs identified from caa(5) and thall(2).

CHECK DUPLICATED
"""

ha_df.duplicated().sum()

ha_df[ha_df.duplicated()]

"""We discover that there are 1 duplicate which is row 164.

CHECK OUTLIERS
"""

ha_df.boxplot()

"""As shown above, there are outliers detected."""

ha_df.head()

categorical_col = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall', 'output'] # CATEGORICAL FEATURES
continouos_col = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak'] # CONTINOUS FEATURES

"""GRAPHS FOR CATEGORICAL DATA"""

for i in categorical_col:
  plt.figure()
  sns.countplot(ha_df[i])
  plt.show()

"""GRAPHS FOR CONTIUNOUS DATA"""

for i in continouos_col:
  plt.figure()
  sns.displot(ha_df[i])
  plt.show()

"""#STEP 3) DATA CLEANING

REMOVE NANs
"""

for i in categorical_col:
  ha_df[i] = ha_df[i].fillna(ha_df[i].mode()[0])

for i in continouos_col:
  ha_df[i] = ha_df[i].fillna(ha_df[i].median())

ha_df.isna().sum()

"""REMOVE DUPLICATES"""

ha_df.duplicated().sum()

ha_df.drop_duplicates

"""#STEP 4) FEATURE SELECTION"""

X = ha_df.drop(labels='output', axis=1)
y = ha_df['output'].astype(int)

selected_features = []

for i in continouos_col:
    lr = LogisticRegression()
    lr.fit(np.expand_dims(X[i],axis=1),y)
    print(i)
    print(lr.score(np.expand_dims(X[i],axis=-1),y))
    if lr.score(np.expand_dims(X[i],axis=-1),y) > 0.6:
        selected_features.append(i)

for i in categorical_col:
    print(i)
    matrix = pd.crosstab(ha_df[i],y).to_numpy()
    print(cramers_corrected_stat(matrix))
    if cramers_corrected_stat(matrix) > 0.4:
        selected_features.append(i)

"""#STEP 5) PREPROCESSING"""

X = ha_df.loc[:,continouos_col]

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.3,
                                                    random_state=123)

"""# MODEL DEVELOPMENT

*Logistic Regression*
"""

pipeline_mms_lr = Pipeline([
                           ('Min_Max_Scaler',MinMaxScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
]) # Pipeline([STEPS])

pipeline_ss_lr = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
]) # Pipeline([STEPS])

"""*Decision Tree*"""

pipeline_mms_dt = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Tree_Classifier', DecisionTreeClassifier())
]) # Pipeline([STEPS])

pipeline_ss_dt = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Tree_Classifier', DecisionTreeClassifier())
]) # Pipeline([STEPS])

"""*Random Forest*"""

pipeline_mms_rf = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Forest_Classifier', RandomForestClassifier())
]) # Pipeline([STEPS])

pipeline_ss_rf = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Forest_Classifier', RandomForestClassifier())
]) # Pipeline([STEPS])

"""*Gradient Boost*"""

pipeline_mms_gb = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('GBoost_Classifier', GradientBoostingClassifier())
]) # Pipeline([STEPS])

pipeline_ss_gb = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('GBoost_Classifier', GradientBoostingClassifier())
]) # Pipeline([STEPS])

"""*SVC*"""

pipeline_mms_svc = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('SVC_Classifier', SVC())
]) # Pipeline([STEPS])

pipeline_ss_svc = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('SVC_Classifier', SVC())
]) # Pipeline([STEPS])

"""*KNN*"""

pipeline_mms_knn = Pipeline([
                            ('Min_Max_Scaler',MinMaxScaler()),
                            ('KNN_Classifier',KNeighborsClassifier(n_neighbors=10))
]) # Pipeline([STEPS])

pipeline_ss_knn = Pipeline([
                            ('Standard_Scaler',StandardScaler()),
                            ('KNN_Classifier',KNeighborsClassifier(n_neighbors=10))
]) # Pipeline([STEPS])

"""To create a list to store all the pipelines"""

pipelines = [pipeline_mms_lr, pipeline_ss_lr,
             pipeline_mms_dt, pipeline_ss_dt,
             pipeline_mms_rf, pipeline_ss_rf,
             pipeline_mms_gb, pipeline_ss_gb,
             pipeline_mms_svc, pipeline_ss_svc,
             pipeline_mms_knn, pipeline_ss_knn]

for pipe in pipelines:
  pipe.fit(X_train, y_train)

for i, pipe in enumerate(pipelines):
  print(pipe.score(X_test, y_test))

best_accuracy = 0

for i, pipe in enumerate(pipelines):
  if pipe.score(X_test, y_test) > best_accuracy:
    best_accuracy = pipe.score(X_test, y_test)
    best_pipeline = pipe

print('The best scaler and classifier for heart dataset is {} with accuracy of {}'
.format(best_pipeline.steps,best_accuracy))

"""GridSearchCV"""

pipeline_ss_svc = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('SVC_Classifier', SVC())
]) # Pipeline([STEPS])

grid_param = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},
              {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]

grid_search = GridSearchCV(estimator = SVC(),
                           param_grid = grid_param,
                           verbose = 1,
                           cv=5,
                           n_jobs=-1)

grid = grid_search.fit(X_train,y_train)
grid.score(X_test, y_test)
print(grid.best_params_)

grid_predictions = grid.predict(X_test)
print(confusion_matrix(y_test,grid_predictions))

print(classification_report(y_test,grid_predictions))

"""# MODEL SAVING"""

BEST_ESTIMATOR_SAVE_PATH=os.path.join(os.getcwd(),'best_estimator.pkl')

with open(BEST_ESTIMATOR_SAVE_PATH, 'wb') as file:
    pickle.dump(grid.best_estimator_,file)